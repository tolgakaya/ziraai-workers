# ============================================
# ZiraAI Analysis Worker Configuration
# Multi-Provider AI Strategy Support
# ============================================

# Worker Identification
WORKER_ID=analysis-worker-001
NODE_ENV=development
LOG_LEVEL=info

# Concurrency & Performance
CONCURRENCY=60
HEALTH_CHECK_INTERVAL=30000
TIMEOUT=60000
RATE_LIMIT=350

# ============================================
# QUEUE SYSTEM SELECTION (Phase 1 Day 5+)
# ============================================
# USE_PROVIDER_QUEUES: Toggle between OLD and NEW queue architecture
#   false (default) - OLD System: Consume from WebAPI direct queues
#                     (plant-analysis-requests, plant-analysis-multi-image-requests)
#   true           - NEW System: Consume from provider-specific queues
#                     (openai-analysis-queue, gemini-analysis-queue, anthropic-analysis-queue)
#
# NOTE: This must match WebAPI's PlantAnalysis:UseRawAnalysisQueue setting
#       WebAPI uses raw queue → Dispatcher routes → Provider queues → Workers
USE_PROVIDER_QUEUES=false

# ============================================
# PROVIDER API KEYS
# ============================================
# At least one provider API key is required
# Multiple providers enable failover and load distribution

# OpenAI (gpt-4o-mini)
OPENAI_API_KEY=sk-...

# Google Gemini (gemini-2.0-flash-exp)
GEMINI_API_KEY=...

# Anthropic Claude (claude-3-5-sonnet)
ANTHROPIC_API_KEY=...

# ============================================
# PROVIDER SELECTION STRATEGY
# ============================================
# ⚠️ CRITICAL: Workers MUST use FIXED strategy when USE_PROVIDER_QUEUES=true
#
# Architecture:
#   - Each worker instance is specialized for ONE provider
#   - Worker consumes from SINGLE provider-specific queue only
#   - Dispatcher handles routing strategies (ROUND_ROBIN, COST_OPTIMIZED, etc.)
#   - This enables independent scaling of worker replicas per provider
#
# FIXED Strategy (REQUIRED for NEW queue system):
#   - Worker uses only one provider's API key
#   - Worker consumes from only one queue:
#     * openai   → openai-analysis-queue
#     * gemini   → gemini-analysis-queue
#     * anthropic → anthropic-analysis-queue
#
# Example: Gemini Worker
PROVIDER_SELECTION_STRATEGY=FIXED
PROVIDER_FIXED=gemini

# Example: OpenAI Worker
# PROVIDER_SELECTION_STRATEGY=FIXED
# PROVIDER_FIXED=openai

# Example: Anthropic Worker
# PROVIDER_SELECTION_STRATEGY=FIXED
# PROVIDER_FIXED=anthropic

# ============================================
# PROVIDER METADATA (Dynamic Cost & Quality)
# ============================================
# Optional: Override default cost and quality scores
# Useful for:
#   - Updated pricing from provider APIs
#   - A/B testing results (quality adjustments)
#   - Custom quality scoring based on domain-specific metrics
#
# Format: JSON object with provider metadata
# PROVIDER_METADATA={"gemini":{"costPerMillion":1.0,"qualityScore":7},"openai":{"costPerMillion":5.0,"qualityScore":8},"anthropic":{"costPerMillion":48.0,"qualityScore":10}}
#
# Default values (if not specified):
#   Gemini:    costPerMillion=1.087,  qualityScore=7  (Good)
#   OpenAI:    costPerMillion=5.125,  qualityScore=8  (Very Good)
#   Anthropic: costPerMillion=48.0,   qualityScore=10 (Excellent)
#
# Example: Adjust Gemini quality score after A/B testing
# PROVIDER_METADATA={"gemini":{"qualityScore":8.5}}
#
# Example: Update all costs after provider price changes
# PROVIDER_METADATA={"gemini":{"costPerMillion":0.9},"openai":{"costPerMillion":4.5},"anthropic":{"costPerMillion":45.0}}

# ============================================
# PROVIDER MODEL CONFIGURATION
# ============================================
# Specify which AI model to use for each provider
# You can use ANY valid model name from each provider

# Gemini Models (default: gemini-2.0-flash-exp)
# GEMINI_MODEL=gemini-2.0-flash-exp  # Latest flash model (recommended)
# GEMINI_MODEL=gemini-1.5-pro        # Pro model for higher quality
# GEMINI_MODEL=gemini-1.5-flash      # Previous flash generation

# Anthropic Models (default: claude-3-5-sonnet-20241022)
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # Latest Sonnet (recommended)
# ANTHROPIC_MODEL=claude-3-opus-20240229      # Opus - highest intelligence
# ANTHROPIC_MODEL=claude-3-haiku-20240307     # Haiku - fastest/cheapest

# OpenAI Models (default: gpt-4o-mini)
PROVIDER_MODEL=gpt-4o-mini  # Recommended for cost/performance balance
# PROVIDER_MODEL=gpt-4o      # More capable, higher cost
# PROVIDER_MODEL=o1-preview  # Reasoning model

# ============================================
# RABBITMQ CONFIGURATION
# ============================================
RABBITMQ_URL=amqp://localhost:5672

# PHASE 1: WebAPI Integration (Current)
# Worker consumes from existing WebAPI queue: plant-analysis-requests
# This queue is already configured in WebAPI appsettings.json
# Worker will automatically create queue if it doesn't exist

# Result and Error Queues
RESULT_QUEUE=plant-analysis-results  # Must match WebAPI configuration
DLQ_QUEUE=analysis-dlq

# Message Processing Configuration
PREFETCH_COUNT=10  # Messages to fetch at once (higher = more concurrency)

# PHASE 2: Provider-Specific Queues (Future)
# When WebAPI is updated with dispatcher service, worker will consume from:
# - openai-analysis-queue: OpenAI GPT-4o-mini requests
# - gemini-analysis-queue: Google Gemini Flash 2.0 requests
# - anthropic-analysis-queue: Claude 3.5 Sonnet requests

# ============================================
# REDIS CONFIGURATION (Rate Limiting)
# ============================================
REDIS_URL=redis://localhost:6379
REDIS_KEY_PREFIX=ziraai:ratelimit:
REDIS_TTL=120

# ============================================
# DEPLOYMENT EXAMPLES
# ============================================

# Example 1: Cost Optimization (Gemini Worker for Lowest Cost)
# Deploy multiple Gemini worker replicas to handle high volume at lowest cost
# PROVIDER_SELECTION_STRATEGY=FIXED
# PROVIDER_FIXED=gemini
# GEMINI_API_KEY=AIza...
# Result: Worker consumes gemini-analysis-queue only ($1.087/1M analyses)

# Example 2: Quality Priority (Anthropic Worker for Best Results)
# Deploy Anthropic worker replicas for premium quality requirements
# PROVIDER_SELECTION_STRATEGY=FIXED
# PROVIDER_FIXED=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# Result: Worker consumes anthropic-analysis-queue only ($48/1M analyses)

# Example 3: Balanced Multi-Provider Setup (Recommended)
# Deploy separate worker instances for each provider, scale independently:
#
# Gemini Workers (3 replicas for high volume):
#   PROVIDER_FIXED=gemini
#   Consumes: gemini-analysis-queue
#
# OpenAI Workers (2 replicas for balanced cost/quality):
#   PROVIDER_FIXED=openai
#   Consumes: openai-analysis-queue
#
# Anthropic Workers (1 replica for premium quality):
#   PROVIDER_FIXED=anthropic
#   Consumes: anthropic-analysis-queue
#
# Dispatcher (1 instance) routes with ROUND_ROBIN or COST_OPTIMIZED strategy

# ============================================
# COST ESTIMATES (per 1M analyses, ~8.5K input + 1.5K output tokens)
# ============================================
# Gemini (flash-2.0):     $1,087 per 1M
# OpenAI (gpt-4o-mini):   $5,125 per 1M
# Anthropic (claude-3.5): $48,000 per 1M

# ============================================
# RAILWAY DEPLOYMENT NOTES
# ============================================
# For Railway deployment, set these environment variables in Railway dashboard
# You can override any setting per environment (Staging, Production)
# 
# Recommended Railway Setup:
# - Staging: FIXED strategy with Gemini (cost testing)
# - Production: ROUND_ROBIN with all providers (reliability + cost balance)
